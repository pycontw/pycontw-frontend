__NUXT_JSONP__("/conference/talk/343", (function(a,b){return {data:[{speechData:{id:343,begin_time:"2025-09-06T02:50:00Z",end_time:"2025-09-06T03:05:00Z",is_remote:b,location:"7-r4",youtube_id:a,title:"Spell it with Sign Language: An Asl Typing Game with MediaPipe",category:"EDU",language:"ENEN",python_level:"INTERMEDIATE",recording_policy:b,abstract:"This talk introduces a Python-based typing game that helps users practice American Sign Language (ASL) fingerspelling using real-time hand tracking and gesture recognition. Players \"type\" letters by signing them into a webcam, with MediaPipe used for hand landmark detection and scikit-learn for static sign classification. For dynamic letters like \"J\" and \"Z,\" we use a second model built with PyTorch to capture motion patterns from sequential data.\r\n\r\nWe'll introduce ASL fingerspelling for context, then walk through our development pipeline—from data collection (both self-recorded and public datasets) to training, evaluation, and game design. A demo will showcase the game, which has been tested by students in educational settings. We’ll share performance metrics (~88% static accuracy at 15 FPS), deployment lessons, and a public GitHub repo to support reproducibility. If you're into computer vision, accessibility, or building fun Python projects, this talk is for you.",detailed_description:"Learning ASL can be interactive and fun—especially when paired with real-time hand tracking and immediate feedback. In this talk, Ethan and Megan present their Python-based game that lets users practice fingerspelling by \"typing\" with hand gestures. Using MediaPipe, the system extracts 21 hand landmarks, feeding them into a trained model to recognize each letter.\r\n\r\nWe’ll first introduce ASL fingerspelling and explain the differences between static and dynamic signs. Then, we’ll cover how we collected data—including over 2,000 samples—and trained classifiers using scikit-learn and PyTorch. We’ll share our design choices, including feedback mechanics, and present a simple results table (~88% accuracy for static signs, ~72% on dynamic, 15–18 FPS on CPU). We'll also share feedback from a classroom pilot with younger students using the tool to learn ASL basics.\r\n\r\nThis talk is for educators, Python developers, and accessibility advocates. You'll leave with code, data resources, and a deeper understanding of how Python and AI can be combined to build inclusive tools.",slide_link:"https:\u002F\u002Fwww.canva.com\u002Fdesign\u002FDAGwzIGXkSY\u002FRtHP-N1EVj8C70asE4KfHg\u002Fedit?utm_content=DAGwzIGXkSY&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton",slido_embed_link:"https:\u002F\u002Fapp.sli.do\u002Fevent\u002FawQwq4WnSQ28guKn2i8PtU",hackmd_embed_link:"https:\u002F\u002Fhackmd.io\u002F@pycontw\u002FSJApa3dYxl",speakers:[{thumbnail_url:"https:\u002F\u002Ftw.pycon.org\u002Fprs\u002Fmedia\u002Fcache\u002Fdb\u002F27\u002Fdb2735d9327d9c04d308c1368affae2f.jpg",name:"Megan & Ethan",github_profile_url:a,twitter_profile_url:a,facebook_profile_url:a,bio:"This team consists of Megan Chang and Ethan Chang. Megan is a senior, while Ethan is a freshman, both studying at Kaohsiung American School. They developed an ASL software that detects sign language using an recognition system, inspired by their volunteer experience at a deaf-mute café. They plan to use this system in the future to help reduce language barriers and raise awareness about the deaf-mute community."}],event_type:"talk"}}],fetch:{},mutations:[]}}("",false)));