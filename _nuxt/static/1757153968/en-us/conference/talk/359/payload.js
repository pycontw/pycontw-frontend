__NUXT_JSONP__("/en-us/conference/talk/359", (function(a,b){a.id=359;a.begin_time="2025-09-06T02:50:00Z";a.end_time="2025-09-06T03:20:00Z";a.is_remote=false;a.location="5-r1";a.youtube_id=b;a.title="Rediscovering Parquet in Python — From CSV Pain to Columnar Gain";a.category="DATA";a.language="ZHEN";a.python_level="INTERMEDIATE";a.recording_policy=true;a.abstract="In this session, we rewind to the pre‑Parquet era—when schemaless text, row‑oriented I\u002FO, and ineffective compression crippled analytics—and then methodically uncovers each design decision that made Parquet the de‑facto columnar format. Illustrate how column‑oriented storage, row‑group statistics, page‑level encodings, and Bloom filters cut data‑scanned by two orders of magnitude. We conclude with a practical comparison of PyArrow, Polars, and DuckDB, demonstrating that modern Python tooling turns these optimizations into one‑liners.";a.detailed_description="With the rise of LLMs and ever-growing datasets in 2025, understanding Parquet’s internals is more valuable than ever for handling data intensive applications and pipelines to have better processing performance.\r\nThis session guides the audience through a structured discovery of Parquet’s architecture. We begin by pains when using CSV at scale, then introduce each Parquet feature exactly when it eliminates the corresponding pain point: columnar buffers, row groups, pages, statistics, and Bloom filters. Each concept is illustrated with clear visuals to show how it effectively resolves the corresponding challenge.\r\nToward the end of the session, we’ll briefly explore how tools like PyArrow, Polars, and DuckDB leverage Parquet data layout to minimize data scanned and CPU time, demonstrating realistic, practical use cases.\r\nThe talk concludes with practical recommendations—row‑group sizing, partition layouts, codec selection that attendees can apply immediately in their own analytics pipelines.";a.slide_link="https:\u002F\u002Fwww.figma.com\u002Fdeck\u002FeE0dQXtGExjvhx0SIDYvgR\u002Fpycontw2025-parquet-intro?node-id=5-91&t=SJnpqIu1G29JRv3x-0&scaling=min-zoom&content-scaling=fixed&page-id=0%3A1";a.slido_embed_link="https:\u002F\u002Fapp.sli.do\u002Fevent\u002Fa3ZFyymKW5uGHaFPcZhW9o";a.hackmd_embed_link="https:\u002F\u002Fhackmd.io\u002F@pycontw\u002FHJrp6huFex";a.speakers=[{thumbnail_url:"https:\u002F\u002Ftw.pycon.org\u002Fprs\u002Fmedia\u002Fcache\u002Fbc\u002Fb6\u002Fbcb682b786880e31907ab396795fa658.jpg",name:"Josix",github_profile_url:"https:\u002F\u002Fgithub.com\u002Fjosix",twitter_profile_url:"https:\u002F\u002Ftwitter.com\u002Fjosixisoj",facebook_profile_url:b,bio:"engineer & tool-builder; PyCon Taiwan staff, contributor to Apache Airflow and Python zh-TW docs. Passionate about DX, data workflows, and practical Python tooling."}];a.event_type="talk";return {data:[{speechData:a}],fetch:{},mutations:[["setSpeechData",a]]}}({},"")));