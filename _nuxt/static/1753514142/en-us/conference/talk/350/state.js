window.__NUXT__=(function(a,b,c,d){return {staticAssetsBase:"\u002Fpycontw-frontend\u002F_nuxt\u002Fstatic\u002F1753514142",layout:"default",error:d,state:{sponsorsData:[],jobsData:[],schedulesData:[],keynotesData:[],youtubeInfo:[],speechesData:[],speechData:{id:350,begin_time:"2025-09-06T06:00:00Z",end_time:"2025-09-06T06:45:00Z",is_remote:c,location:"5-r1",youtube_id:b,title:"Apache Airflow: Synchronizing Datasets across Multiple instances",category:"APPL",language:"ENEN",python_level:"INTERMEDIATE",recording_policy:a,abstract:"Data Engineer jobs regularly include scheduling or scaling workflows.\r\n\r\nBut have you ever asked yourself, can I **scale my scheduling** ?\r\n\r\nIt turns out that you can!\r\nBut doing so raises a number of issues that need to be addressed.\r\n\r\nIn this talk we will be:\r\n - Resuming Data-aware scheduling in Apache Airflow\r\n - Discussing diverse methods to upscale our scheduler\r\n - Choosing a specific solution to synchronize Airflow Datasets between instances\r\n - Discussing some roadblocks and limitations that we have met along the way\r\n\r\nAll of this in the context of a professional production environment.\r\n\r\nThank you for your time, I hope you enjoy the talk.",detailed_description:"**Feedback adjustments:**\r\n\r\nAs per reviewer #FB70's request I have increased the time and effort spent on introducing Apache Airflow with the following program:\r\n- Resuming Data-aware scheduling in Apache Airflow (10m)\r\n - Intro to Apache Airflow\r\n     - Orchestrator\r\n     - Open Source & Community\r\n     - DAGs & DAG Runs\r\n - Airflow Scheduling\r\n     - Fixed Schedule\r\n     - Data-Aware Scheduling\r\n     - Not Event Scheduling\r\n\r\nAs per reviewer #671A's suggestion I have incorporated Airflow 3 to my presentation with the following:\r\n - Presenting the Airflow 3 update and the vocabulary change of Datasets to Assets.\r\n - Presenting the built-in, data event polling solution from [AIP-82](https:\u002F\u002Fcwiki.apache.org\u002Fconfluence\u002Fdisplay\u002FAIRFLOW\u002FAIP-82+External+event+driven+scheduling+in+Airflow)\r\n - Comparing the strengths and weaknesses of this solution vs my external solution\r\n     - Polling, built-in and more load on Airflow server vs Pushing, external services, monitoring and more work\r\n\r\n\r\n**Detailed description:**\r\n\r\n- WhoAmI (30s): A brief description of my career and my experience. This will answer the basic questions of who am I and Why should you listen to me talk about Airflow.\r\n\r\n- **Resuming Data-aware scheduling in Apache Airflow** (10m)\r\n - Intro to Apache Airflow\r\n     - Orchestrator: Also called Job Scheduler. What it is and how it works.\r\n     - Open Source & Community: Give some historical details on the project as well as its strength as an open source project. Plus some metrics from [OSS Rank](https:\u002F\u002Fossrank.com\u002Fp\u002F6-apache-airflow).\r\n     - DAGs & DAG Runs: Jobs are represented by Directed Acyclic Graphs. These are then instantiated into DAG Runs for every execution that is scheduled.\r\n - Airflow Scheduling\r\n     - Fixed Schedule: Regular, fixed-interval scheduling. Mostly represented by crontabs, but you can use timedeltas if you prefer. (I do not plan to talk about advanced concepts such as time tables)\r\n     - Data-Aware Scheduling: Represent data sources with a specific key. Make links between DAGs that affect a data source and DAGs that are affected by it. Whenever that data source is updated, we make an event with the source's key. This then tells all affected DAGs about the change. They can then Run if needed.\r\n     - Not Event Scheduling: Clear distinction between the two. Event Scheduling has a 1 for 1 event-run relation. Data-Aware Scheduling only guarantees that a DAG will run for the latest change. You don't care how many changes a Data source has received, only that it has changed since the DAG last ran.\r\n\r\n- Discussing diverse methods to upscale our scheduler (6m)\r\n - When is scaling needed ?: More DAGs, more frequency, more parallelism means more processing power and more business criticity from Airflow. paying for a better server or cloud instance works for smaller needs, but eventually.\r\n - Airflow Executors: Using an external cluster to do the brunt of the workload, such as with the KubernetesExecutor is a good way to reduce the load on the Airflow server. If other processes aren't taking recources from it, it can work faster. \r\n - Scheduler Replicas: Requires good technical knowledge on the DB side, especially if you aren't using the recommended PostgreSQL 12+ or MySQL 8.0+. But this does allow you to scale scheduling very well, as long as your DB isn't overloaded by the Airflow activity. \r\n - Separate Instances: Once a critcal mass of DAGs forms, it is natural to want to split them into seperate instances. Although this solves the problem, it also introduces an inevitable choice of what thematic link to we split over? Who is in charge? In my company we first split by stakeholder teams, before regretting our choice and shuffling back into a split by DAG functionality. This was because it was too difficult to ensure stakeholder teams had similar levels of Airflow know-how.\r\n\r\n- Solving the issue of maintaining our Airflow Datasets synchronized between instances (15m)\r\n - Describing the working environment : We've two Airflow Instances, one in 2.4 one in 2.10 with both having hundreds of jobs. When one creates a new Dataset Event, we'd like the event to be propagated to the other. And vice-versa. \r\n - Different implementations and their respective tradeoffs\r\n   - Using custom \"high-frequency\" DAGs: Quick and Dirty, but has big downsides such as depending on Airflow workload, having an easily accessible pause button, and scaling the number of exchanges to n^2, with n being the number of Airflow Instances.\r\n   - Using Airflow Listeners: Listeners are objects that wait until a specific event happens to run. These are great, but they run directly on the scheduler so we have to keep them light and error management might be an issue. Also n^2 scaling.\r\n   - Using an external service via the Airflow API: This solution requires an external message queue and external processes that feed it and push from it. But the polling applications should be simple consumers and producers plugged into the Airflow API. And the number of exchanges only scales to 2n. \r\n\r\n- Roadblocks (8m)\r\n - Airflow API issues (version & Dataset Creation): In Airflow 2.4 the API does not have a Dataset Event creation endpoint. I also discovered that the Dataset Event Post (creation) endpoint does not work for Datasets that do not already exist in the DB (404). \r\n - Airflow parsed object vs DB objects: Difference between DAGs and Datasets, which are parsed from DAG definitions and can dynamically change depending on the codebase, and DAG Runs, Variables and Connections which are stored in the DB and do not change unless the DB is modified. Parsed objects cannot be created via API call in Airflow 2.\r\n - DatasetAlias: This is a Dataset Placeholder that allows you to dynamically decided which Dataset and how many Datasets a DAG is the producer or consumer of. It really saved this project.\r\n - Airflow Listeners: This feature does not exist for Dataset Events, only for Datasets. I'm going to work on it as soon as I get Listeners to work on my setup. Actually if I can find some Airflow maintainers during Pyconn TW to give me some tips, that would be great.\r\n\r\n- Describing the current solution (5min):\r\n\r\nWe'll address the solution as the ADS: Airflow Dataset Synchronizer\r\n\r\nUsing:\r\n - Kafka Topic as the central Dataset Event channel\r\n - A DatasetAlias producing DAG on each instance\r\n - One kafka producer process per Airflow instance: Reads from Airflow API with a save point inside a Variable, pushes to Kafka with some additional extras.\r\n - One kafka consumer process per Airflow instance: Reads from Kafka, consumer group garantees new data only, filter Dataset Events to avoir repushing events from an instance on itself. Splits Dataset Events into new and seen. New are pushed to DatasetAlias job via Airflow API. Seen are pushed directly through API.\r\n\r\n\r\nPresent the functional loop of a dataset event being created and sent through the ADS.\r\n\r\nPros:\r\n - Dataset events can be produced by external systems as well\r\n - No Airflow activity unless necessary\r\n - It was fun making this and I learned new thigns about Airflow\r\n\r\nCons:\r\n - Monitoring and maintainance\r\n - Complexity\r\n\r\n- Changes in Airflow3.0 (described in Feedback Adjustments at the top)\r\n    - (Data) Assets\r\n    - A built-in solution in AIP-82\r\n    - Pull based Asset sync vs push based Asset sync\r\n\r\n\r\n- Airflow usage in production @ Numberly (Bonus if there is time left): I will present my company's Airflow usage, with charts and numbers.",slide_link:b,slido_embed_link:b,hackmd_embed_link:b,speakers:[{thumbnail_url:"https:\u002F\u002Ftw.pycon.org\u002Fprs\u002Fmedia\u002Fcache\u002F09\u002Fa0\u002F09a0caed6119ce10d850932417a81b54.jpg",name:"Sebastien Crocquevieille",github_profile_url:b,twitter_profile_url:b,facebook_profile_url:b,bio:"Who:\r\n - Data Engineer currently working in Taiwan\r\n - French & Mexican\r\n - Speak EN, FR, ES & some ZH\r\n\r\nWhat:\r\n - Hadoop, PySpark, Kubernetes\r\n - Open Source enthusiast\r\n - Tech for Good\r\n - Basketball & Video Games\r\n\r\nWhere:\r\n - Europython 2019-2024\r\n - Pycon TW 2023"}],event_type:"talk"},relatedData:[],reviewerData:[],configs:{conferenceName:"PyCon TW",conferenceYear:"2025",conferenceDate:"2025-09-06",showAboutStaffPage:c,showConferencePage:a,showSchedulePage:a,showEventOverviewPage:a,showEventsPage:a,showIndexSecondaryBtn:a,showIndexSponsorSection:c,showProposalSystemPage:a,showRegistrationPage:a,showSpeakingPage:a,showSponsorPage:a,showVenuePage:c,aboutHideItems:["apacCommunity"],conferenceHideItems:[],eventsHideItems:["jobs"],registrationHideItems:[],venueHideItems:["venueInfo","accommodation"]},i18n:{routeParams:{}}},serverRendered:a,routePath:"\u002Fen-us\u002Fconference\u002Ftalk\u002F350",config:{http:{browserBaseURL:"https:\u002F\u002Fstaging.pycon.tw\u002Fprs"},gtm:{id:"GTM-TNZ39PD"},_app:{basePath:"\u002Fpycontw-frontend\u002F",assetsPath:"\u002Fpycontw-frontend\u002F_nuxt\u002F",cdnURL:d}}}}(true,"",false,null));